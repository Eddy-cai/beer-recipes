---
title: "Data analysis on beer recipes"
author: "Junjie Cai"
date: "13 June 2018"
output: html_document
---

## Abstract

## Introduction
Machine learning and statistical analysis have been widely studied and applied in a variety of fields in recent years, especially in business area. Companies get benefits from these techniques, thus wining the competition. Therefore, it is common and necessary to analyze features of the product by employing data mining and statistical methods. The types of problems that machine learning algorithms are applied typically consist of *classification* problem, which predict two or more discrete outcomes, and *regression* problems, which predict continues values.

In this project, a set of supervised learning methods are employed in a classification problem related to home-brewed beer recipes. The data comes from Kaggle.com website, and contain a set of variables referring to features in the fermatation of beers, thus it is a good data source to find realtionship between these features. Each observation is classified as either _dark_ beer or _light_ beer. The goal of this project is going to predict the final color.

## Data

_Beer recipes_ dataset is hosted in Kaggle.com website mentioned above, which is a website running data science competations. The dataset is or iginal sourced from the [Brewer's Friend](https://www.brewersfriend.com/search/)  website, which is a place for users to share their homebrew beers. Users are allowed to upload their homebrew beer recipes by entering values for each feature. It is possible that users just enter whatever they want.

### Data pre-process
This original dataset contains 73,861 observations of 21 variables, as shown in **Appendix, Table N**. However, it is necessary to pre process the initial dataset, as most of 21 variables are either not applicable or have no contribution on the goal of the project (e.g., *URL* just refers to the link of the item in [Brewer's Friend](https://www.brewersfriend.com/search/) website).

For pre-processing, 5 most useful variables are selected from the original dataset, and hence make up a new dataset with 63,949 observations of 6 columns, as shown in **Appendix, Table N**.

Further, in order to obtain a good result, the quantitative variable *Color* is transformed to the qualitative variable *Dark* with two values ("yes" or "no"). Each observation with the value of *color* greater than 25 is classified into "yes" lable, otherwise "no". Then, we remove *Color* variable.

After that, we extract the same number of items with "no" lable as "yes" lable, which benefits to reduce variance. finally, we obtain the dataset which will be analyzed in this project, as shown in  **Appendix, Table N**.

### Feature analysis
Here we are going to roughly look at the relationship between 4 features and _dark_ variable, as shown in Figture 1.
```{r}
# plot boxplot of Dark and other features
par
```


## Methods

### Analysis methods

### Models and algorithm

#### Random guessing

#### Logistic regression

#### Logistic regression with non-linear transformation

#### Linear discriminant analysis

#### Decision tree

#### k-nearest neighbor algorithm

## Results and evaluation

## Conclusion

I am your sdasdasd
